{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre_onboarding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from math import log\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "oiDsy13Mu2F0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 1"
      ],
      "metadata": {
        "id": "UAKpt8J4sEMc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3w-WFxHoeEcq"
      },
      "outputs": [],
      "source": [
        "class Tokenizer() :\n",
        "  def __init__(self) :\n",
        "    self.word_dict = {'oov' : 0}\n",
        "    self.fit_checker = False\n",
        "\n",
        "  def preprocessing(self,sequences) :\n",
        "    result = []\n",
        "    for sequence in sequences :\n",
        "      token = re.sub(r\"[^a-zA-Z0-9]\",\" \",sequence) #정규표현식\n",
        "      token = token.lower().split()\n",
        "      result.append(token)\n",
        "    return result\n",
        "\n",
        "  def fit(self, sequences) :\n",
        "    self.fit_checker = False\n",
        "    tokenized = self.preprocessing(sequences)\n",
        "    for token in tokenized :\n",
        "      for word in token :\n",
        "        if word not in self.word_dict :\n",
        "          self.word_dict[word] = len(self.word_dict)\n",
        "    self.fit_checker = True\n",
        "\n",
        "  def transform(self, sequences) :\n",
        "    result = []\n",
        "    tokenized = self.preprocessing(sequences)\n",
        "    if self.fit_checker :\n",
        "      for token in tokenized :\n",
        "        temp = []\n",
        "        for word in token :\n",
        "          if word in self.word_dict :\n",
        "            temp.append(self.word_dict[word])\n",
        "          else :\n",
        "            temp.append(self.word_dict['oov'])\n",
        "        result.append(temp)\n",
        "\n",
        "      return result\n",
        "    else :\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "\n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "input = ['I go to school',\"I LIKE pizza!\"]\n",
        "\n",
        "\n",
        "print(tokenizer.preprocessing(input))\n",
        "\n",
        "tokenizer.fit(input)\n",
        "print(tokenizer.word_dict)\n",
        "print(tokenizer.transform(input))"
      ],
      "metadata": {
        "id": "rt9Yz1BfV415",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc0a791-c509-407a-a7bc-319e9c382922"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
            "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6}\n",
            "[[1, 2, 3, 4], [1, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 2"
      ],
      "metadata": {
        "id": "iGBi7pAor_7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer():\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "\n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    word_dict = self.tokenizer.word_dict\n",
        "\n",
        "    idf_list = []\n",
        "    n = len(tokenized)\n",
        "    for tokens in range(len(word_dict)) :\n",
        "      df = 0\n",
        "      for words in tokenized :\n",
        "          if tokens in words :\n",
        "            df += 1\n",
        "          idf = np.log(n/(df+1))\n",
        "      idf_list.append(idf)\n",
        "    return idf_list\n",
        "    self.fit_checker = True\n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      vocab = list(self.tokenizer.word_dict())\n",
        "      token_doc = self.tokenizer.preprocessing(sequences)\n",
        "\n",
        "      n = len(tokenized)\n",
        "      tf = []\n",
        "      for i in range(n) :\n",
        "        tf.append([])\n",
        "        d = token_doc[i]\n",
        "        for j in range(len(vocab)) :\n",
        "          t = vocab[j]\n",
        "          tf[-1].append(d.count(t))\n",
        "\n",
        "        self.tfidf = (np.array(tf) * self.idf_list)\n",
        "        return self.tfidf\n",
        "      else:\n",
        "        raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)\n",
        "    \n",
        "\n",
        "\n",
        "                  \n",
        "    "
      ],
      "metadata": {
        "id": "lDcyVeDnwyJg"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "* https://zsunn.tistory.com/entry/AI-%EB%B2%A1%ED%84%B0%ED%99%94-%EB%B0%8F-One-Hot-Encoding-%EC%8B%A4%EC%8A%B5?category=891504\n",
        "\n",
        "* https://cheris8.github.io/data%20analysis/TP-Encoding/\n",
        "\n",
        "* https://stackoverflow.com/questions/59748251/class-implementation-in-python-tf-idf-cf\n",
        "\n",
        "* https://wikidocs.net/31698\n"
      ],
      "metadata": {
        "id": "ALUqWbVzPNC5"
      }
    }
  ]
}